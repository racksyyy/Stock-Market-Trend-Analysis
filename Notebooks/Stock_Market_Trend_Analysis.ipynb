{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3d92463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (2.4.1)\n",
      "Requirement already satisfied: yfinance in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (1.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (2.3.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (3.10.8)\n",
      "Requirement already satisfied: xgboost in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (3.1.3)\n",
      "Requirement already satisfied: nltk in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (3.9.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (1.17.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (1.8.0)\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.20.0-cp313-cp313-win_amd64.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (1.5.3)\n",
      "Requirement already satisfied: h5py in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (3.15.1)\n",
      "Requirement already satisfied: requests>=2.31 in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from yfinance) (2.32.5)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from yfinance) (0.0.12)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from yfinance) (4.5.1)\n",
      "Requirement already satisfied: pytz>=2022.5 in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from yfinance) (2025.2)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from yfinance) (2.4.7)\n",
      "Requirement already satisfied: peewee>=3.16.2 in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from yfinance) (3.18.3)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from yfinance) (4.14.3)\n",
      "Requirement already satisfied: curl_cffi<0.14,>=0.7 in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from yfinance) (0.13.0)\n",
      "Requirement already satisfied: protobuf>=3.19.0 in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from yfinance) (6.33.2)\n",
      "Requirement already satisfied: websockets>=13.0 in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from yfinance) (15.0.1)\n",
      "Requirement already satisfied: cffi>=1.12.0 in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from curl_cffi<0.14,>=0.7->yfinance) (2.0.0)\n",
      "Requirement already satisfied: certifi>=2024.2.2 in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from curl_cffi<0.14,>=0.7->yfinance) (2026.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from matplotlib) (12.1.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from matplotlib) (3.3.1)\n",
      "Requirement already satisfied: click in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from nltk) (8.3.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tensorflow) (25.12.19)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tensorflow) (0.7.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tensorflow) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tensorflow) (2.0.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tensorflow) (1.76.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests>=2.31->yfinance) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests>=2.31->yfinance) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests>=2.31->yfinance) (2.6.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.5)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: soupsieve>=1.6.1 in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.8.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from cffi>=1.12.0->curl_cffi<0.14,>=0.7->yfinance) (2.23)\n",
      "Requirement already satisfied: rich in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from keras>=3.10.0->tensorflow) (14.2.0)\n",
      "Requirement already satisfied: namex in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from keras>=3.10.0->tensorflow) (0.18.0)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\prana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n",
      "Using cached tensorflow-2.20.0-cp313-cp313-win_amd64.whl (332.0 MB)\n",
      "Installing collected packages: tensorflow\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\prana\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python313\\\\site-packages\\\\tensorflow\\\\include\\\\external\\\\com_github_grpc_grpc\\\\src\\\\core\\\\ext\\\\filters\\\\fault_injection\\\\fault_injection_service_config_parser.h'\n",
      "HINT: This error might have occurred since this system does not have Windows Long Path support enabled. You can find information on how to enable this at https://pip.pypa.io/warnings/enable-long-paths\n",
      "\n",
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\prana\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade numpy yfinance pandas matplotlib xgboost nltk scipy scikit-learn tensorflow joblib h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25841fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš  Warning: TensorFlow/Keras not found. Please install: pip install tensorflow\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import yfinance as yf\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score, mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# TensorFlow/Keras imports for LSTM\n",
    "_tf_available = False\n",
    "try:\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "    _tf_available = True\n",
    "    print(\"âœ“ TensorFlow/Keras imported successfully\")\n",
    "except ImportError as e1:\n",
    "    try:\n",
    "        from keras.models import Sequential\n",
    "        from keras.layers import LSTM, Dense, Dropout\n",
    "        from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "        _tf_available = True\n",
    "        print(\"âœ“ Keras imported successfully\")\n",
    "    except ImportError as e2:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"âŒ ERROR: TensorFlow/Keras is not installed or not importable!\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"To fix this issue:\")\n",
    "        print(\"1. Install TensorFlow: %pip install tensorflow\")\n",
    "        print(\"2. If you get Windows Long Path errors, enable Long Path support:\")\n",
    "        print(\"   - Run PowerShell as Administrator\")\n",
    "        print(\"   - Execute: New-ItemProperty -Path 'HKLM:\\SYSTEM\\CurrentControlSet\\Control\\FileSystem' -Name 'LongPathsEnabled' -Value 1 -PropertyType DWORD -Force\")\n",
    "        print(\"   - Restart your computer\")\n",
    "        print(\"3. Alternative: Try installing with: %pip install tensorflow-cpu\")\n",
    "        print(\"=\"*70)\n",
    "        raise ImportError(\"TensorFlow/Keras is required for LSTM models but could not be imported. \"\n",
    "                         f\"TensorFlow error: {e1}, Standalone Keras error: {e2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d72a499d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating resilient download for 50 tickers...\n",
      "Downloading batch: ['NVDA', 'AAPL', 'GOOG', 'MSFT', 'AMZN', 'META', 'TSLA', 'AVGO', '2222.SR', 'TSM', 'BRK-B', 'LLY', 'WMT', 'JPM', 'TCEHY']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  15 of 15 completed\n",
      "[*************         27%                       ]  4 of 15 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading batch: ['V', 'ORCL', 'MA', '005930.KS', 'XOM', 'JNJ', 'PLTR', 'BAC', 'ASML', 'ABBV', 'NFLX', '601288.SS', 'COST', 'MC.PA', 'BABA']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  15 of 15 completed\n",
      "[******                13%                       ]  2 of 15 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading batch: ['1398.HK', 'AMD', 'HD', '601939.SS', 'ROG.SW', 'PG', 'GE', 'MU', 'CSCO', 'KO', 'WFC', 'CVX', 'UNH', 'MS', 'SAP']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  15 of 15 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading batch: ['TM', 'AZN', 'IBM', 'CAT', '000660.KS']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  5 of 5 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tickers downloaded successfully.\n",
      "âœ“ Data ingestion complete. File saved: d:\\VSCode\\Projects\\Stock-Market-Trend-Analysis\\Data\\market_data_raw.csv\n",
      "âœ“ Successfully collected 50 tickers\n"
     ]
    }
   ],
   "source": [
    "TOP_50_TICKERS = [\n",
    "    \"NVDA\", \"AAPL\", \"GOOG\", \"MSFT\", \"AMZN\", \"META\", \"TSLA\", \"AVGO\", \"2222.SR\", \"TSM\",\n",
    "    \"BRK-B\", \"LLY\", \"WMT\", \"JPM\", \"TCEHY\", \"V\", \"ORCL\", \"MA\", \"005930.KS\", \"XOM\",\n",
    "    \"JNJ\", \"PLTR\", \"BAC\", \"ASML\", \"ABBV\", \"NFLX\", \"601288.SS\", \"COST\", \"MC.PA\", \"BABA\",\n",
    "    \"1398.HK\", \"AMD\", \"HD\", \"601939.SS\", \"ROG.SW\", \"PG\", \"GE\", \"MU\", \"CSCO\", \"KO\",\n",
    "    \"WFC\", \"CVX\", \"UNH\", \"MS\", \"SAP\", \"TM\", \"AZN\", \"IBM\", \"CAT\", \"000660.KS\"\n",
    "]\n",
    "\n",
    "current_dir = Path.cwd()\n",
    "project_root = current_dir if current_dir.name != 'Notebooks' else current_dir.parent\n",
    "raw_data_path = project_root / \"Data\" \n",
    "raw_data_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def fetch_tickers_in_batches(tickers, batch_size=10, period=\"6mo\"):\n",
    "    all_data = []\n",
    "    for i in range(0, len(tickers), batch_size):\n",
    "        batch = tickers[i:i + batch_size]\n",
    "        print(f\"Downloading batch: {batch}\")\n",
    "        try:\n",
    "            data = yf.download(batch, period=period, group_by='ticker', auto_adjust=True, threads=True)\n",
    "            if not data.empty:\n",
    "                all_data.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Batch download error: {e}\")\n",
    "        time.sleep(1)\n",
    "    return pd.concat(all_data, axis=1) if all_data else pd.DataFrame()\n",
    "\n",
    "def robust_downloader(tickers, period=\"6mo\", max_retries=3):\n",
    "    print(f\"Initiating resilient download for {len(tickers)} tickers...\")\n",
    "    df = fetch_tickers_in_batches(tickers, batch_size=15, period=period)\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        existing_tickers = df.columns.get_level_values(0).unique().tolist()\n",
    "        failed_tickers = [t for t in tickers if t not in existing_tickers]\n",
    "        \n",
    "        if not failed_tickers:\n",
    "            print(\"All tickers downloaded successfully.\")\n",
    "            break\n",
    "            \n",
    "        wait_time = (attempt + 1) * 5\n",
    "        print(f\"Attempt {attempt + 1}/{max_retries}: {len(failed_tickers)} failures. Retrying in {wait_time}s...\")\n",
    "        time.sleep(wait_time)\n",
    "        \n",
    "        for ticker in failed_tickers:\n",
    "            try:\n",
    "                retry_data = yf.download(ticker, period=period, auto_adjust=True, progress=False)\n",
    "                if not retry_data.empty and len(retry_data) > 20:\n",
    "                    # Fix MultiIndex issue\n",
    "                    if not isinstance(retry_data.columns, pd.MultiIndex):\n",
    "                        retry_data.columns = pd.MultiIndex.from_product([[ticker], retry_data.columns])\n",
    "                    df = pd.concat([df, retry_data], axis=1)\n",
    "                    print(f\"âœ“ Successfully retrieved {ticker}\")\n",
    "            except Exception as e:\n",
    "                print(f\"âœ— Failed for {ticker}: {str(e)[:50]}\")\n",
    "\n",
    "    df = df.sort_index().interpolate(method='time').ffill().bfill()\n",
    "    \n",
    "    missing_final = [t for t in tickers if t not in df.columns.get_level_values(0).unique()]\n",
    "    if missing_final:\n",
    "        print(f\"âš  CRITICAL: Still missing {len(missing_final)} tickers: {missing_final}\")\n",
    "    \n",
    "    file_path = raw_data_path / \"market_data_raw.csv\"\n",
    "    df.to_csv(file_path)\n",
    "    print(f\"âœ“ Data ingestion complete. File saved: {file_path}\")\n",
    "    print(f\"âœ“ Successfully collected {df.columns.get_level_values(0).nunique()} tickers\")\n",
    "    return df\n",
    "\n",
    "raw_df = robust_downloader(TOP_50_TICKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b47c2594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 6950\n",
      "Tickers: 50\n",
      "Date range: 2025-08-04 to 2026-01-12\n",
      "\n",
      "Rows per ticker:\n",
      "count     50.0\n",
      "mean     139.0\n",
      "std        0.0\n",
      "min      139.0\n",
      "25%      139.0\n",
      "50%      139.0\n",
      "75%      139.0\n",
      "max      139.0\n",
      "Name: count, dtype: float64\n",
      "\n",
      "Missing values:\n",
      "Date              0\n",
      "Open              0\n",
      "High              0\n",
      "Low               0\n",
      "Close             0\n",
      "Volume            0\n",
      "RSI_14            0\n",
      "SMA_5             0\n",
      "SMA_20            0\n",
      "SMA_Ratio_5_20    0\n",
      "MACD              0\n",
      "Signal            0\n",
      "MACD_Histogram    0\n",
      "BB_Upper          0\n",
      "BB_Lower          0\n",
      "BB_Position       0\n",
      "Vol_SMA_20        0\n",
      "Volume_Ratio      0\n",
      "Ticker            0\n",
      "Anomaly_Flag      0\n",
      "Is_Anomaly        0\n",
      "Cluster           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Add this cell to check your data:\n",
    "df = pd.read_csv(proc_dir / \"market_data_clusters.csv\")\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "print(f\"Tickers: {df['Ticker'].nunique()}\")\n",
    "print(f\"Date range: {df['Date'].min()} to {df['Date'].max()}\")\n",
    "print(f\"\\nRows per ticker:\")\n",
    "print(df['Ticker'].value_counts().describe())\n",
    "print(f\"\\nMissing values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd6d9ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw data and calculating features...\n",
      "âœ“ Processed META: 140 days\n",
      "âœ“ Processed GOOG: 140 days\n",
      "âœ“ Processed AVGO: 140 days\n",
      "âœ“ Processed TSLA: 140 days\n",
      "âœ“ Processed BRK-B: 140 days\n",
      "âœ“ Processed TCEHY: 140 days\n",
      "âœ“ Processed AMZN: 140 days\n",
      "âœ“ Processed 2222.SR: 140 days\n",
      "âœ“ Processed NVDA: 140 days\n",
      "âœ“ Processed JPM: 140 days\n",
      "âœ“ Processed TSM: 140 days\n",
      "âœ“ Processed LLY: 140 days\n",
      "âœ“ Processed AAPL: 140 days\n",
      "âœ“ Processed MSFT: 140 days\n",
      "âœ“ Processed WMT: 140 days\n",
      "âœ“ Processed BABA: 140 days\n",
      "âœ“ Processed PLTR: 140 days\n",
      "âœ“ Processed NFLX: 140 days\n",
      "âœ“ Processed 601288.SS: 140 days\n",
      "âœ“ Processed ORCL: 140 days\n",
      "âœ“ Processed ASML: 140 days\n",
      "âœ“ Processed COST: 140 days\n",
      "âœ“ Processed ABBV: 140 days\n",
      "âœ“ Processed V: 140 days\n",
      "âœ“ Processed 005930.KS: 140 days\n",
      "âœ“ Processed MC.PA: 140 days\n",
      "âœ“ Processed MA: 140 days\n",
      "âœ“ Processed XOM: 140 days\n",
      "âœ“ Processed BAC: 140 days\n",
      "âœ“ Processed JNJ: 140 days\n",
      "âœ“ Processed MS: 140 days\n",
      "âœ“ Processed AMD: 140 days\n",
      "âœ“ Processed SAP: 140 days\n",
      "âœ“ Processed UNH: 140 days\n",
      "âœ“ Processed WFC: 140 days\n",
      "âœ“ Processed CSCO: 140 days\n",
      "âœ“ Processed 601939.SS: 140 days\n",
      "âœ“ Processed 1398.HK: 140 days\n",
      "âœ“ Processed PG: 140 days\n",
      "âœ“ Processed KO: 140 days\n",
      "âœ“ Processed MU: 140 days\n",
      "âœ“ Processed CVX: 140 days\n",
      "âœ“ Processed ROG.SW: 140 days\n",
      "âœ“ Processed GE: 140 days\n",
      "âœ“ Processed HD: 140 days\n",
      "âœ“ Processed AZN: 140 days\n",
      "âœ“ Processed 000660.KS: 140 days\n",
      "âœ“ Processed CAT: 140 days\n",
      "âœ“ Processed TM: 140 days\n",
      "âœ“ Processed IBM: 140 days\n",
      "\n",
      "âœ“ Total: 7000 rows across 50 tickers\n",
      "âœ“ File saved: d:\\VSCode\\Projects\\Stock-Market-Trend-Analysis\\Data\\market_data_features.csv\n"
     ]
    }
   ],
   "source": [
    "current_dir = Path.cwd()\n",
    "project_root = current_dir if current_dir.name != 'Notebooks' else current_dir.parent\n",
    "raw_file = project_root / \"Data\" / \"market_data_raw.csv\"\n",
    "proc_dir = project_root / \"Data\" \n",
    "proc_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def calculate_indicators(df):\n",
    "    \"\"\"Calculates technical indicators\"\"\"\n",
    "    df = df.copy()  # Avoid SettingWithCopyWarning\n",
    "    \n",
    "    # RSI_14\n",
    "    delta = df['Close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    rs = gain / loss\n",
    "    df['RSI_14'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "    # SMA Ratio 5/20\n",
    "    df['SMA_5'] = df['Close'].rolling(window=5).mean()\n",
    "    df['SMA_20'] = df['Close'].rolling(window=20).mean()\n",
    "    df['SMA_Ratio_5_20'] = df['SMA_5'] / df['SMA_20']\n",
    "\n",
    "    # MACD Histogram\n",
    "    ema12 = df['Close'].ewm(span=12, adjust=False).mean()\n",
    "    ema26 = df['Close'].ewm(span=26, adjust=False).mean()\n",
    "    df['MACD'] = ema12 - ema26\n",
    "    df['Signal'] = df['MACD'].ewm(span=9, adjust=False).mean()\n",
    "    df['MACD_Histogram'] = df['MACD'] - df['Signal']\n",
    "\n",
    "    # Bollinger Bands Position\n",
    "    std = df['Close'].rolling(window=20).std()\n",
    "    df['BB_Upper'] = df['SMA_20'] + (std * 2)\n",
    "    df['BB_Lower'] = df['SMA_20'] - (std * 2)\n",
    "    df['BB_Position'] = (df['Close'] - df['BB_Lower']) / (df['BB_Upper'] - df['BB_Lower'])\n",
    "\n",
    "    # Volume Ratio\n",
    "    df['Vol_SMA_20'] = df['Volume'].rolling(window=20).mean()\n",
    "    df['Volume_Ratio'] = df['Volume'] / df['Vol_SMA_20']\n",
    "\n",
    "    return df.dropna()\n",
    "\n",
    "print(\"Loading raw data and calculating features...\")\n",
    "df_raw = pd.read_csv(raw_file, header=[0, 1], index_col=0, parse_dates=True)\n",
    "tickers = df_raw.columns.get_level_values(0).unique()\n",
    "\n",
    "all_processed = []\n",
    "for ticker in tickers:\n",
    "    try:\n",
    "        stock_data = df_raw[ticker].copy()\n",
    "        if stock_data.empty or len(stock_data) < 50:\n",
    "            print(f\"âš  Skipping {ticker}: Insufficient data ({len(stock_data)} days)\")\n",
    "            continue \n",
    "\n",
    "        processed_stock = calculate_indicators(stock_data)\n",
    "        processed_stock['Ticker'] = ticker  # No more SettingWithCopyWarning\n",
    "        all_processed.append(processed_stock)\n",
    "        print(f\"âœ“ Processed {ticker}: {len(processed_stock)} days\")\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Error processing {ticker}: {str(e)[:50]}\")\n",
    "\n",
    "final_df = pd.concat(all_processed)\n",
    "final_df.to_csv(proc_dir / \"market_data_features.csv\")\n",
    "print(f\"\\nâœ“ Total: {len(final_df)} rows across {final_df['Ticker'].nunique()} tickers\")\n",
    "print(f\"âœ“ File saved: {proc_dir / 'market_data_features.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bedef5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 350 anomalies across all tickers.\n"
     ]
    }
   ],
   "source": [
    "anomaly_df = pd.read_csv(proc_dir / \"market_data_features.csv\")\n",
    "model_root = project_root / \"Models\"\n",
    "model_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "features = [\n",
    "    'RSI_14',\n",
    "    'SMA_Ratio_5_20',\n",
    "    'MACD_Histogram',\n",
    "    'BB_Position',\n",
    "    'Volume_Ratio'\n",
    "]\n",
    "\n",
    "clean_anomaly_df = anomaly_df.dropna(subset=features).copy()\n",
    "\n",
    "iso_model = IsolationForest(n_estimators=100, max_samples='auto', contamination=0.05, random_state=42, max_features=len(features), n_jobs=-1)\n",
    "\n",
    "clean_anomaly_df['Anomaly_Flag'] = iso_model.fit_predict(clean_anomaly_df[features])\n",
    "clean_anomaly_df['Is_Anomaly'] = clean_anomaly_df['Anomaly_Flag'].apply(lambda x: 1 if x == -1 else 0)\n",
    "\n",
    "joblib.dump(iso_model, model_root / \"isolation_forest_model.pkl\")\n",
    "clean_anomaly_df.to_csv(proc_dir / \"market_data_anomalies.csv\", index=False)\n",
    "\n",
    "print(f\"Detected {clean_anomaly_df['Is_Anomaly'].sum()} anomalies across all tickers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edd4ce6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ticker  Cluster  Distance     RSI_14  SMA_Ratio_5_20  MACD_Histogram  \\\n",
      "29  MC.PA        2  0.548191  61.123779        1.011473        0.412827   \n",
      "12    AZN        2  0.572248  56.227176        1.012099        0.041974   \n",
      "40    SAP        2  0.830896  52.612893        1.010896        0.911332   \n",
      "\n",
      "    BB_Position  Volume_Ratio  \n",
      "29     0.750263      1.133350  \n",
      "12     0.665719      1.248418  \n",
      "40     0.775155      1.421570  \n"
     ]
    }
   ],
   "source": [
    "clustering_df = pd.read_csv(proc_dir / \"market_data_anomalies.csv\")\n",
    "clean_df = clustering_df.dropna(subset=features).copy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(clean_df[features])\n",
    "\n",
    "optimal_k = 5\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "clean_df['Cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "model_root = project_root / \"Models\"\n",
    "model_root.mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump(kmeans, model_root / \"kmeans_model.pkl\")\n",
    "joblib.dump(scaler, model_root / \"scaler.pkl\")\n",
    "clean_df.to_csv(proc_dir / \"market_data_clusters.csv\", index=False)\n",
    "\n",
    "def find_similar_tickers(query_features_dict, df, kmeans_model, scaler_model, top_n=5):\n",
    "    query_df = pd.DataFrame([query_features_dict])\n",
    "    query_scaled = scaler_model.transform(query_df[features])\n",
    "    \n",
    "    query_cluster = kmeans_model.predict(query_scaled)[0]\n",
    "    \n",
    "    latest_states = df.sort_values('Date').groupby('Ticker').last().reset_index()\n",
    "    cluster_peers = latest_states[latest_states['Cluster'] == query_cluster].copy()\n",
    "    \n",
    "    if cluster_peers.empty:\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    peer_features_scaled = scaler_model.transform(cluster_peers[features])\n",
    "    distances = cdist(query_scaled, peer_features_scaled, metric='euclidean')[0]\n",
    "    \n",
    "    cluster_peers['Distance'] = distances\n",
    "    return cluster_peers.nsmallest(top_n, 'Distance')[['Ticker', 'Cluster', 'Distance'] + features]\n",
    "\n",
    "query_features = {\n",
    "    'RSI_14': 65.5, \n",
    "    'SMA_Ratio_5_20': 1.02, \n",
    "    'MACD_Histogram': 0.5,\n",
    "    'BB_Position': 0.7, \n",
    "    'Volume_Ratio': 1.3\n",
    "}\n",
    "\n",
    "similar = find_similar_tickers(query_features, clean_df, kmeans, scaler, top_n=3)\n",
    "print(similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c575953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directional Accuracy: 56.64%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['d:\\\\VSCode\\\\Projects\\\\Stock-Market-Trend-Analysis\\\\Models\\\\trend_model.pkl']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(proc_dir / \"market_data_clusters.csv\")\n",
    "df['Target'] = (df.groupby('Ticker')['Close'].shift(-1) > df['Close']).astype(int)\n",
    "features = [\n",
    "    'RSI_14', \n",
    "    'MACD_Histogram',\n",
    "    'BB_Position', \n",
    "    'SMA_Ratio_5_20',\n",
    "    'Volume_Ratio', \n",
    "    'Is_Anomaly', \n",
    "    'Cluster'\n",
    "]\n",
    "clean_df = df.dropna(subset=['Target'] + features).copy()\n",
    "split_idx = int(len(clean_df) * 0.8)\n",
    "train_df = clean_df.iloc[:split_idx]\n",
    "test_df = clean_df.iloc[split_idx:]\n",
    "X_train, y_train = train_df[features], train_df['Target']\n",
    "X_test, y_test = test_df[features], test_df['Target']\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=500,learning_rate=0.05,max_depth=5,random_state=42,eval_metric='logloss')\n",
    "xgb_model.fit(X_train, y_train,eval_set=[(X_test, y_test)],verbose=False)\n",
    "\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Directional Accuracy: {acc:.2%}\")\n",
    "\n",
    "joblib.dump(xgb_model, model_root / \"trend_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7bdd088f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "LSTM TIME SERIES PREDICTION FOR MULTIPLE TICKERS\n",
      "======================================================================\n",
      "\n",
      "âœ“ Loaded data: 7000 rows, 50 tickers\n",
      "âœ“ Date range: 2025-08-04 00:00:00 to 2026-01-13 00:00:00\n",
      "\n",
      "======================================================================\n",
      "Processing 50 tickers...\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 95\u001b[39m\n\u001b[32m     91\u001b[39m y_test_ticker = y[split_idx:]\n\u001b[32m     93\u001b[39m \u001b[38;5;66;03m# Build LSTM model for this ticker\u001b[39;00m\n\u001b[32m     94\u001b[39m model = Sequential([\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m     \u001b[43mLSTM\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_sequences\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLOOKBACK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfeature_cols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m     96\u001b[39m     Dropout(\u001b[32m0.2\u001b[39m),\n\u001b[32m     97\u001b[39m     LSTM(\u001b[32m50\u001b[39m, return_sequences=\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[32m     98\u001b[39m     Dropout(\u001b[32m0.2\u001b[39m),\n\u001b[32m     99\u001b[39m     LSTM(\u001b[32m50\u001b[39m),\n\u001b[32m    100\u001b[39m     Dropout(\u001b[32m0.2\u001b[39m),\n\u001b[32m    101\u001b[39m     Dense(\u001b[32m25\u001b[39m),\n\u001b[32m    102\u001b[39m     Dense(FORECAST_HORIZON)\n\u001b[32m    103\u001b[39m ])\n\u001b[32m    105\u001b[39m model.compile(optimizer=\u001b[33m'\u001b[39m\u001b[33madam\u001b[39m\u001b[33m'\u001b[39m, loss=\u001b[33m'\u001b[39m\u001b[33mmse\u001b[39m\u001b[33m'\u001b[39m, metrics=[\u001b[33m'\u001b[39m\u001b[33mmae\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m    107\u001b[39m \u001b[38;5;66;03m# Callbacks\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LSTM MODEL FOR MULTIPLE TICKERS - TIME SERIES PREDICTION\n",
    "# ============================================================================\n",
    "# Note: All required imports are in Cell 1\n",
    "\n",
    "# Load the processed data\n",
    "current_dir = Path.cwd()\n",
    "project_root = current_dir if current_dir.name != 'Notebooks' else current_dir.parent\n",
    "proc_dir = project_root / \"Data\"\n",
    "model_root = project_root / \"Models\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LSTM TIME SERIES PREDICTION FOR MULTIPLE TICKERS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(proc_dir / \"market_data_clusters.csv\")\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df = df.sort_values(['Ticker', 'Date']).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nâœ“ Loaded data: {len(df)} rows, {df['Ticker'].nunique()} tickers\")\n",
    "print(f\"âœ“ Date range: {df['Date'].min()} to {df['Date'].max()}\")\n",
    "\n",
    "# Feature selection for LSTM\n",
    "feature_cols = [\n",
    "    'Open', 'High', 'Low', 'Close', 'Volume',\n",
    "    'RSI_14', 'SMA_Ratio_5_20', 'MACD_Histogram', \n",
    "    'BB_Position', 'Volume_Ratio'\n",
    "]\n",
    "\n",
    "# Function to create sequences for LSTM\n",
    "def create_sequences(data, lookback=60, forecast_horizon=1):\n",
    "    \"\"\"\n",
    "    Create sequences for LSTM training\n",
    "    Args:\n",
    "        data: numpy array of shape (n_samples, n_features)\n",
    "        lookback: number of time steps to look back\n",
    "        forecast_horizon: number of steps ahead to predict\n",
    "    Returns:\n",
    "        X: sequences of shape (n_sequences, lookback, n_features)\n",
    "        y: targets of shape (n_sequences, forecast_horizon)\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - lookback - forecast_horizon + 1):\n",
    "        X.append(data[i:i+lookback])\n",
    "        y.append(data[i+lookback:i+lookback+forecast_horizon, 3])  # Close price is at index 3\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Prepare data for each ticker\n",
    "LOOKBACK = 60  # Use 60 days to predict next day\n",
    "FORECAST_HORIZON = 1  # Predict 1 day ahead\n",
    "\n",
    "all_X_train, all_y_train = [], []\n",
    "all_X_test, all_y_test = [], []\n",
    "ticker_scalers = {}\n",
    "ticker_models = {}\n",
    "\n",
    "tickers = df['Ticker'].unique()\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Processing {len(tickers)} tickers...\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Process each ticker\n",
    "for ticker in tickers:\n",
    "    ticker_data = df[df['Ticker'] == ticker].copy()\n",
    "    \n",
    "    if len(ticker_data) < LOOKBACK + 20:  # Need minimum data\n",
    "        print(f\"âš  Skipping {ticker}: Insufficient data ({len(ticker_data)} days)\")\n",
    "        continue\n",
    "    \n",
    "    # Extract features\n",
    "    ticker_features = ticker_data[feature_cols].values\n",
    "    \n",
    "    # Scale the data (per ticker)\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    ticker_scaled = scaler.fit_transform(ticker_features)\n",
    "    ticker_scalers[ticker] = scaler\n",
    "    \n",
    "    # Create sequences\n",
    "    X, y = create_sequences(ticker_scaled, LOOKBACK, FORECAST_HORIZON)\n",
    "    \n",
    "    if len(X) < 20:  # Need minimum sequences\n",
    "        print(f\"âš  Skipping {ticker}: Insufficient sequences ({len(X)})\")\n",
    "        continue\n",
    "    \n",
    "    # Split train/test (80/20) - maintaining temporal order\n",
    "    split_idx = int(len(X) * 0.8)\n",
    "    X_train_ticker = X[:split_idx]\n",
    "    y_train_ticker = y[:split_idx]\n",
    "    X_test_ticker = X[split_idx:]\n",
    "    y_test_ticker = y[split_idx:]\n",
    "    \n",
    "    # Build LSTM model for this ticker\n",
    "    model = Sequential([\n",
    "        LSTM(50, return_sequences=True, input_shape=(LOOKBACK, len(feature_cols))),\n",
    "        Dropout(0.2),\n",
    "        LSTM(50, return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        LSTM(50),\n",
    "        Dropout(0.2),\n",
    "        Dense(25),\n",
    "        Dense(FORECAST_HORIZON)\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    \n",
    "    # Callbacks\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    \n",
    "    # Train the model\n",
    "    print(f\"\\nðŸ“Š Training LSTM for {ticker}...\")\n",
    "    print(f\"   Training samples: {len(X_train_ticker)}, Test samples: {len(X_test_ticker)}\")\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train_ticker, y_train_ticker,\n",
    "        validation_data=(X_test_ticker, y_test_ticker),\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    train_pred = model.predict(X_train_ticker, verbose=0)\n",
    "    test_pred = model.predict(X_test_ticker, verbose=0)\n",
    "    \n",
    "    # Inverse transform predictions (denormalize)\n",
    "    # Create dummy arrays for inverse transform\n",
    "    train_pred_full = np.zeros((len(train_pred), len(feature_cols)))\n",
    "    train_pred_full[:, 3] = train_pred.flatten()  # Close price at index 3\n",
    "    train_pred_denorm = scaler.inverse_transform(train_pred_full)[:, 3]\n",
    "    \n",
    "    test_pred_full = np.zeros((len(test_pred), len(feature_cols)))\n",
    "    test_pred_full[:, 3] = test_pred.flatten()\n",
    "    test_pred_denorm = scaler.inverse_transform(test_pred_full)[:, 3]\n",
    "    \n",
    "    # Denormalize actual values\n",
    "    y_train_full = np.zeros((len(y_train_ticker), len(feature_cols)))\n",
    "    y_train_full[:, 3] = y_train_ticker.flatten()\n",
    "    y_train_denorm = scaler.inverse_transform(y_train_full)[:, 3]\n",
    "    \n",
    "    y_test_full = np.zeros((len(y_test_ticker), len(feature_cols)))\n",
    "    y_test_full[:, 3] = y_test_ticker.flatten()\n",
    "    y_test_denorm = scaler.inverse_transform(y_test_full)[:, 3]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train_denorm, train_pred_denorm))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test_denorm, test_pred_denorm))\n",
    "    train_mae = mean_absolute_error(y_train_denorm, train_pred_denorm)\n",
    "    test_mae = mean_absolute_error(y_test_denorm, test_pred_denorm)\n",
    "    test_r2 = r2_score(y_test_denorm, test_pred_denorm)\n",
    "    \n",
    "    print(f\"   âœ“ Train RMSE: ${train_rmse:.2f}, MAE: ${train_mae:.2f}\")\n",
    "    print(f\"   âœ“ Test  RMSE: ${test_rmse:.2f}, MAE: ${test_mae:.2f}, RÂ²: {test_r2:.4f}\")\n",
    "    \n",
    "    # Store model and data\n",
    "    ticker_models[ticker] = model\n",
    "    all_X_train.append(X_train_ticker)\n",
    "    all_y_train.append(y_train_denorm)\n",
    "    all_X_test.append(X_test_ticker)\n",
    "    all_y_test.append(y_test_denorm)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"âœ“ Successfully trained LSTM models for {len(ticker_models)} tickers\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Aggregate results across all tickers\n",
    "if len(ticker_models) > 0:\n",
    "    all_test_predictions = []\n",
    "    all_test_actuals = []\n",
    "    \n",
    "    for ticker in ticker_models.keys():\n",
    "        ticker_data = df[df['Ticker'] == ticker].copy()\n",
    "        ticker_features = ticker_data[feature_cols].values\n",
    "        scaler = ticker_scalers[ticker]\n",
    "        ticker_scaled = scaler.transform(ticker_features)\n",
    "        X, y = create_sequences(ticker_scaled, LOOKBACK, FORECAST_HORIZON)\n",
    "        \n",
    "        split_idx = int(len(X) * 0.8)\n",
    "        X_test_ticker = X[split_idx:]\n",
    "        y_test_ticker = y[split_idx:]\n",
    "        \n",
    "        model = ticker_models[ticker]\n",
    "        test_pred = model.predict(X_test_ticker, verbose=0)\n",
    "        \n",
    "        # Denormalize\n",
    "        test_pred_full = np.zeros((len(test_pred), len(feature_cols)))\n",
    "        test_pred_full[:, 3] = test_pred.flatten()\n",
    "        test_pred_denorm = scaler.inverse_transform(test_pred_full)[:, 3]\n",
    "        \n",
    "        y_test_full = np.zeros((len(y_test_ticker), len(feature_cols)))\n",
    "        y_test_full[:, 3] = y_test_ticker.flatten()\n",
    "        y_test_denorm = scaler.inverse_transform(y_test_full)[:, 3]\n",
    "        \n",
    "        all_test_predictions.extend(test_pred_denorm)\n",
    "        all_test_actuals.extend(y_test_denorm)\n",
    "    \n",
    "    # Overall metrics\n",
    "    overall_rmse = np.sqrt(mean_squared_error(all_test_actuals, all_test_predictions))\n",
    "    overall_mae = mean_absolute_error(all_test_actuals, all_test_predictions)\n",
    "    overall_r2 = r2_score(all_test_actuals, all_test_predictions)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"OVERALL MODEL PERFORMANCE (All Tickers Combined)\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Test RMSE: ${overall_rmse:.2f}\")\n",
    "    print(f\"Test MAE:  ${overall_mae:.2f}\")\n",
    "    print(f\"Test RÂ²:   {overall_r2:.4f}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Save models\n",
    "    print(f\"\\nðŸ’¾ Saving models...\")\n",
    "    for ticker, model in ticker_models.items():\n",
    "        model_path = model_root / f\"lstm_model_{ticker}.h5\"\n",
    "        model.save(model_path)\n",
    "    \n",
    "    # Save scalers\n",
    "    scaler_path = model_root / \"lstm_scalers.pkl\"\n",
    "    joblib.dump(ticker_scalers, scaler_path)\n",
    "    \n",
    "    print(f\"âœ“ Models saved to {model_root}\")\n",
    "    \n",
    "    # Visualization for a sample ticker\n",
    "    sample_ticker = list(ticker_models.keys())[0]\n",
    "    print(f\"\\nðŸ“ˆ Generating visualization for sample ticker: {sample_ticker}\")\n",
    "    \n",
    "    ticker_data = df[df['Ticker'] == sample_ticker].copy()\n",
    "    ticker_features = ticker_data[feature_cols].values\n",
    "    scaler = ticker_scalers[sample_ticker]\n",
    "    ticker_scaled = scaler.transform(ticker_features)\n",
    "    X, y = create_sequences(ticker_scaled, LOOKBACK, FORECAST_HORIZON)\n",
    "    \n",
    "    split_idx = int(len(X) * 0.8)\n",
    "    X_test_ticker = X[split_idx:]\n",
    "    y_test_ticker = y[split_idx:]\n",
    "    \n",
    "    model = ticker_models[sample_ticker]\n",
    "    test_pred = model.predict(X_test_ticker, verbose=0)\n",
    "    \n",
    "    # Denormalize\n",
    "    test_pred_full = np.zeros((len(test_pred), len(feature_cols)))\n",
    "    test_pred_full[:, 3] = test_pred.flatten()\n",
    "    test_pred_denorm = scaler.inverse_transform(test_pred_full)[:, 3]\n",
    "    \n",
    "    y_test_full = np.zeros((len(y_test_ticker), len(feature_cols)))\n",
    "    y_test_full[:, 3] = y_test_ticker.flatten()\n",
    "    y_test_denorm = scaler.inverse_transform(y_test_full)[:, 3]\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(y_test_denorm, label='Actual Close Price', alpha=0.7, linewidth=2)\n",
    "    plt.plot(test_pred_denorm, label='Predicted Close Price', alpha=0.7, linewidth=2)\n",
    "    plt.title(f'LSTM Predictions vs Actual - {sample_ticker}', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Time Steps', fontsize=12)\n",
    "    plt.ylabel('Close Price ($)', fontsize=12)\n",
    "    plt.legend(fontsize=11)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nâœ… LSTM implementation complete!\")\n",
    "else:\n",
    "    print(\"\\nâš  No models were trained. Check data requirements.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
